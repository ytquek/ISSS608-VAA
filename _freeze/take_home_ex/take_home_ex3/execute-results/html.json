{
  "hash": "1d84b3e927cb8455985084432fad924c",
  "result": {
    "markdown": "---\ntitle: \"Take Home 3\"\nauthor: \"You Ting QUEK\"\ndate: \"17 May, 2024\"\ndate-modified: last-modified\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\n---\n\n\n# **The Source**\n\nThe task is taken from the [VAST Challenge 2024](https://vast-challenge.github.io/2024/). Questions from [Mini Case 1](https://vast-challenge.github.io/2024/MC1.html) will be completed.\n\n# **The Task**\n\nApply appropriate visual analytics methods to help FishEye, a non-profit organization that focuses on illegal fishing, to better identify bias, track behavior changes, and infer temporal patterns from the knowledge graphs prepared by their data analysts.\n\nThis take home exercise is done in conjunction with the group project. My group members are [Keke](https://isss608keke.netlify.app/) and [Alicia Loh](https://lnealicia-isss608.netlify.app/).\n\n## Background\n\nIn Oceanus, commercial fishing is a major part of the local economy, and the activities of local fishing companies are often discussed in local news reports, especially when there is a major disruption or controversial issue. Lately, the news is filled with stories of companies who are willing to break the law and fish illegally.\n\nFishEye International is an organization whose mission is to discover and stop illegal, unreported, and unregulated fishing. FishEye is an independent non-profit, but they share their findings with law enforcement when warranted. FishEye analysts collect open-source data, including news articles and public reports, and have recently started extracting embedded knowledge from these free text sources using several advanced language models. Knowledge from multiple sources is combined to form CatchNet: the Oceanus Knowledge Graph, which is used to search for evidence of possible illegal fishing activity. Analysts know that data may be biased so they review and edit extracted information before it is added to CatchNet. Data for this challenge includes both the extracted and analyst modified knowledge graph and the original source articles.\n\nRecently, the commercial fishing community in Oceanus was rocked with scandal after SouthSeafood Express Corp was caught fishing illegally. The company's leaders claim an innocent mistake, while others in the industry have suggested getting caught was overdue and is likely part of a larger pattern of suspect behavior. FishEye analysts are aware that a polarizing event like this is likely to attract biased perspectives. They are looking for your help to identify sources of bias in their data. They are having particular trouble determining whether bias comes from the source article, the algorithms used to extract the knowledge graph, or possibly somewhere else...\n\n## Tasks and Questions:\n\nYour task is to develop visual analytics approaches that FishEye analysts can use to verify the facts included in their knowledge graph are representative of facts stated in the source text. Analysts should be able to compare consistency of the extracted knowledge with the source and identify and trace sources of bias in the data. Novel use of large language models (LLMs) as part of a visual analytics process is encouraged.\n\n1.  Use novel visualizations and visual analytic workflows to examine the bias in each news source. Create visualizations to help FishEye analysts understand how bias in the original sources changes over time. You may use the knowledge graph extracts and may use a large language model to supplement your understanding.\n\n2.  FishEye uses two LLM extraction algorithms: ShadGPT and BassLine. Develop visualizations to compare the bias of each algorithm. Though not required, you may develop your own LLM-based extraction and include it in the comparison.\n\n3.  FishEye is also interested in understanding the reliability of their human analysts. Use visual analytics to examine potential analyst bias. Provide visual examples of the types of bias present.\n\n4.  Identify unreliable actors: news sources, algorithms, or analysts. Use visualizations to provide evidence for your conclusions. Can you use the data provided and a visual analytics workflow to determine who else may be involved?\n\nNote: the VAST challenge is focused on visual analytics and graphical figures should be included with your response to each question. Please include a reasonable number of figures for each question (no more than about 6) and keep written responses as brief as possible (around 250 words per question). Participants are encouraged to new visual representations rather than relying on traditional or existing approaches.\n\n# Getting Started\n\n## Installing and loading the required libraries\n\nNote: Ensure that the [pacman](https://cran.r-project.org/web/packages/pacman/) package has already been installed.\n\nThe following R packages will be used:\n\n-   tidytext, tidyverse\n\n-   readtext\n\n-   quanteda\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidytext, readtext, quanteda, tidytext, tidyverse)\n```\n:::\n\n\n## **Importing Multiple Text Files**\n\n### **Creating a folder list**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_folder <- \"data/articles/\"\n```\n:::\n\n\n### **Define a function to read all files from a folder into a data frame**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data <- readtext(paste0(\"data/articles\",\"/*\"))\n# alternate version: text_data <- readtext(\"data/articles/*\")\n```\n:::\n\n\nCheck dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_text <- corpus(text_data)\nsummary(corpus_text, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n```\n:::\n:::\n\n\n### **Text Data Processing**\n\n-    [`unnest_tokens()`](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens\n\n-    [`stop_words()`](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tokenize the text column\nunnest_words <- text_data %>%   \n  unnest_tokens(output = word, input = text) \n\n# Filter tokens to include only alphabetic characters and remove stop words\nunnest_words <- filter(unnest_words, \n                       str_detect(word, \"[a-z']$\"),          \n                       !word %in% stop_words$word)\n```\n:::\n\n\nView dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(unnest_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50,147\nColumns: 2\n$ doc_id <chr> \"Alvarez PLC__0__0__Haacklee Herald.txt\", \"Alvarez PLC__0__0__H…\n$ word   <chr> \"marine\", \"sanctuary\", \"aid\", \"boosts\", \"alvarez\", \"plc's\", \"su…\n```\n:::\n:::\n\n\n### Explore Common words\n\nThe code chunk below calculates individual word frequencies to explore common words in the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunnest_words %>%\n  count(word,sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nreadtext object consisting of 3261 documents and 0 docvars.\n# A data frame: 3,261 × 3\n  word             n text     \n  <chr>        <int> <chr>    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,255 more rows\n```\n:::\n:::\n\n\n::: callout-note\nCommon words from the articles seem to be related to sustainable fishing practices in the larger fishing industry and trade.\n:::\n\nView dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(unnest_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 50,147\nColumns: 2\n$ doc_id <chr> \"Alvarez PLC__0__0__Haacklee Herald.txt\", \"Alvarez PLC__0__0__H…\n$ word   <chr> \"marine\", \"sanctuary\", \"aid\", \"boosts\", \"alvarez\", \"plc's\", \"su…\n```\n:::\n:::\n\n\n::: callout-note\nIn the dataset, it can be seen that under the \"doc_id\" column, the company mentioned in the article, along with its author is separated by some underscores and numbers. We will split this column into \"Company\" and \"News_Source\" to simplify later analysis.\n:::\n\n### Split doc_id column\n\nThe column has the format **`Company__Number__Number__News_Source.txt`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_split <- text_data %>%\n  separate_wider_delim(\"doc_id\",\n                       delim = \"__\",\n                       names = c(\"Company\", \"Number\", \"Number2\", \"News_Source\"),\n                       too_many = \"debug\")\n```\n:::\n\n\nView dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(text_data_split)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 338\nColumns: 9\n$ Company          <chr> \"Alvarez PLC\", \"Alvarez PLC\", \"Alvarez PLC\", \"Alvarez…\n$ Number           <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\"…\n$ Number2          <chr> \"0\", \"0\", \"0\", \"1\", \"1\", \"1\", \"0\", \"0\", \"0\", \"1\", \"1\"…\n$ News_Source      <chr> \"Haacklee Herald.txt\", \"Lomark Daily.txt\", \"The News …\n$ doc_id           <chr> \"Alvarez PLC__0__0__Haacklee Herald.txt\", \"Alvarez PL…\n$ doc_id_ok        <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,…\n$ doc_id_pieces    <int> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ doc_id_remainder <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ text             <chr> \"Marine Sanctuary Aid Boosts Alvarez PLC's Sustainabl…\n```\n:::\n:::\n\n\nDrop unnecessary columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_clean <- text_data_split %>%\n  select(-Number, -Number2, -doc_id, -doc_id_ok, -doc_id_pieces, -doc_id_remainder)\n```\n:::\n\n\nView dataframe\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(text_data_clean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 338\nColumns: 3\n$ Company     <chr> \"Alvarez PLC\", \"Alvarez PLC\", \"Alvarez PLC\", \"Alvarez PLC\"…\n$ News_Source <chr> \"Haacklee Herald.txt\", \"Lomark Daily.txt\", \"The News Buoy.…\n$ text        <chr> \"Marine Sanctuary Aid Boosts Alvarez PLC's Sustainable Fis…\n```\n:::\n:::\n\n\n## EDA\n\nList of Company and News Sources\n\n::: panel-tabset\n## Companies\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unique values in the \"Company\" column\nunique_companies <- unique(text_data_clean$Company)\nunique_companies\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Alvarez PLC\"                          \n [2] \"Anderson, Brown and Green\"            \n [3] \"Arellano Group\"                       \n [4] \"Barnes and Sons\"                      \n [5] \"Barnett Ltd\"                          \n [6] \"Bell, Reynolds and Forbes\"            \n [7] \"Bishop-Hernandez\"                     \n [8] \"Blackwell, Clark and Lam\"             \n [9] \"Bowers Group\"                         \n[10] \"Brown, Clarke and Martinez\"           \n[11] \"Burns Inc\"                            \n[12] \"Cain, Simpson and Hernandez\"          \n[13] \"Castillo-Elliott\"                     \n[14] \"Cervantes-Kramer\"                     \n[15] \"Cisneros-Meyer\"                       \n[16] \"Clark-Leon\"                           \n[17] \"Clarke, Scott and Sloan\"              \n[18] \"Clements, Allen and Sullivan\"         \n[19] \"Collins, Johnson and Lloyd\"           \n[20] \"Cook PLC\"                             \n[21] \"Cooper, Holland and Nelson\"           \n[22] \"Craig Ltd\"                            \n[23] \"Cuevas PLC\"                           \n[24] \"Davis-Boyd\"                           \n[25] \"Evans-Pearson\"                        \n[26] \"Flores Ltd\"                           \n[27] \"Franco-Stuart\"                        \n[28] \"Frank Group\"                          \n[29] \"Frey Inc\"                             \n[30] \"Glover, Moran and Johnson\"            \n[31] \"Greer-Holder\"                         \n[32] \"Harper Inc\"                           \n[33] \"Harrell-Walters\"                      \n[34] \"Harrington Inc\"                       \n[35] \"Henderson, Hall and Lutz\"             \n[36] \"Hernandez-Rojas\"                      \n[37] \"Hines-Douglas\"                        \n[38] \"Horn and Sons\"                        \n[39] \"Hughes-Clark\"                         \n[40] \"Jackson Inc\"                          \n[41] \"Jones Group\"                          \n[42] \"Jones, Davis and Grant\"               \n[43] \"Kelly-Smith\"                          \n[44] \"Klein LLC\"                            \n[45] \"Lutz-Fleming\"                         \n[46] \"Mann, Myers and Rivera\"               \n[47] \"Martin LLC\"                           \n[48] \"Martinez-Le\"                          \n[49] \"Mcgee and Sons\"                       \n[50] \"Mclaughlin-Chandler\"                  \n[51] \"Montoya Group\"                        \n[52] \"Moore-Simon\"                          \n[53] \"Murphy, Marshall and Pope\"            \n[54] \"Murray, Friedman and Wall\"            \n[55] \"Namorna Transit Ltd\"                  \n[56] \"NyanzaRiver Worldwide AS\"             \n[57] \"Oka Seafood Shipping Ges.m.b.H.\"      \n[58] \"Olsen Group\"                          \n[59] \"Phelps, Brown and Wallace\"            \n[60] \"Phillips-Newton\"                      \n[61] \"PregolyaDredge Logistics Incorporated\"\n[62] \"Ramos-Shelton\"                        \n[63] \"Rasmussen, Nelson and King\"           \n[64] \"Rhodes-Thompson\"                      \n[65] \"Rivas-Stevens\"                        \n[66] \"Rosario-Melendez\"                     \n[67] \"Roth, Logan and Moreno\"               \n[68] \"Sanchez-Moreno\"                       \n[69] \"Serrano-Cruz\"                         \n[70] \"Smith-Hull\"                           \n[71] \"Solis-Lopez\"                          \n[72] \"Spencer, Richards and Wilson\"         \n[73] \"Taylor, Prince and Sherman\"           \n[74] \"Thomas-Weaver\"                        \n[75] \"Thompson-Padilla\"                     \n[76] \"Turner-Green\"                         \n[77] \"Underwood Inc\"                        \n[78] \"V. Miesel Shipping\"                   \n[79] \"Valdez, Dalton and Cook\"              \n[80] \"Vargas-Jensen\"                        \n[81] \"Vasquez, Chaney and Martinez\"         \n[82] \"Walker, Erickson and Blake\"           \n[83] \"Watson-Gray\"                          \n[84] \"Wilcox-Nelson\"                        \n[85] \"Wu-Hart\"                              \n[86] \"York-Castillo\"                        \n```\n:::\n:::\n\n\n## News Sources\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Unique values in the \"News_Source\" column\nunique_news_sources <- unique(text_data_clean$News_Source)\nunique_news_sources\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Haacklee Herald.txt\"        \"Lomark Daily.txt\"          \n[3] \"The News Buoy.txt\"          \"Haacklee Herald_Police.txt\"\n[5] \"Lomark Daily_Police.txt\"    \"The News Buoy_Police.txt\"  \n```\n:::\n:::\n\n:::\n\n::: callout-note\nThere are a total of:\n\n-   85 companies being mentioned in the news articles\n\n-   6 news outlets that authored the news articles\n:::\n\nDiscovering the company that is mentioned the greatest number of times, and the author of the most news articles\n\n::: panel-tabset\n## Companies\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count the frequency of values in the \"Company\" column and sort by frequency\ncompany_freq <- text_data_clean %>%\n  count(Company) %>%\n  mutate(Percentage = n / sum(n) * 100) %>%\n  arrange(desc(n))\ncompany_freq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 86 × 3\n   Company                        n Percentage\n   <chr>                      <int>      <dbl>\n 1 Cervantes-Kramer              12       3.55\n 2 Jones Group                   12       3.55\n 3 Rasmussen, Nelson and King     8       2.37\n 4 Alvarez PLC                    6       1.78\n 5 Anderson, Brown and Green      6       1.78\n 6 Bell, Reynolds and Forbes      6       1.78\n 7 Blackwell, Clark and Lam       6       1.78\n 8 Brown, Clarke and Martinez     6       1.78\n 9 Burns Inc                      6       1.78\n10 Castillo-Elliott               6       1.78\n# ℹ 76 more rows\n```\n:::\n:::\n\n\n## News Sources\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count the frequency of values in the \"News_Source\" column and sort by frequency\nnews_source_freq <- text_data_clean %>%\n  count(News_Source) %>%\n  mutate(Percentage = n / sum(n) * 100) %>%\n  arrange(desc(n))\nnews_source_freq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n  News_Source                    n Percentage\n  <chr>                      <int>      <dbl>\n1 The News Buoy.txt            112     33.1  \n2 Haacklee Herald.txt          109     32.2  \n3 Lomark Daily.txt             107     31.7  \n4 Haacklee Herald_Police.txt     6      1.78 \n5 Lomark Daily_Police.txt        2      0.592\n6 The News Buoy_Police.txt       2      0.592\n```\n:::\n:::\n\n:::\n\n::: callout-note\nThe top 3 companies with the most mentions in the news articles are:\n\n-   Cervantes-Kramer (12 - 33.1%)\n\n-   Jones GRoup and Rasmussen (12 - 32.2%)\n\n-   Nelson and King (8 - 31.7%)\n\nThe top 3 news outlets with the most number of news articles are:\n\n-   The News Buoy (112 - 33.1%)\n\n-   Haacklee Herald (109 - 32.2%)\n\n-   Lomark Daily (107 - 31.7%)\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}