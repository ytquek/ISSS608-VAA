---
title: "Take Home Exercise 1"
author: "You Ting Quek"
date: "April 15, 2024"
date-modified: "Last-Modified"
execute: 
  eval: true
  echo: true
  warning: false
  freeze: true
---

# Assignment Context

There are two major residential property market in Singapore, namely public and private housing. Public housing aims to meet the basic need of the general public with monthly household income less than or equal to S\$14,000. For families with monthly household income more than S\$14,000, they need to turn to the private residential market.

# The Task

Assuming the role of a graphical editor of a median company, you are requested to prepare minimum two and maximum three data visualisation to reveal the private residential market and sub-markets of Singapore for the 1st quarter of 2024

## Data Source

To accomplish the task, transaction data of REALIS (2023-2024) will be used.

# Research

Prior to kickstarting the project, the author must first do some preliminary research on the local property market in Singapore as well as the factors that may contribute to it's pricing.

The data provided by Realis are at it's crux Transaction data, the author upon further research understands there are other determinant factors such as :

1.) Regions <https://www.propertyguru.com.sg/property-guides/ccr-ocr-rcr-region-singapore-ura-map-21045>

which contributes significantly to the

## Setting up the environment

### Installing required packages

```{r}
pacman::p_load(tidyverse, ggplot2, dplyr, shiny, bslib)
```

## Preparing the data

### Raw Data Import

Given that there are 5 sets of Transaction CSV files, we will need to open each and every single one of them using read_csv function before merging them together again into 1 data frame.

::: panel-tabset
## Code

```{r}
#| eval: false
# Define the paths to the individual CSV files
file1 <- "data/ResidentialTransaction20240308160536.csv"
file2 <- "data/ResidentialTransaction20240308160736.csv"
file3 <- "data/ResidentialTransaction20240308161009.csv"
file4 <- "data/ResidentialTransaction20240308161109.csv"
file5 <- "data/ResidentialTransaction20240414220633.csv"

# Reading the individual CSV files
data1 <- read_csv(file1)
data2 <- read_csv(file2)
data3 <- read_csv(file3)
data4 <- read_csv(file4)
data5 <- read_csv(file5)

# Combining the data frames into one
combined_transaction <- bind_rows(data1, data2, data3, data4, data5)

# Viewing the data structure given
col_names <- names(combined_transaction)
col_names
```

## Output

```{r}
# Define the paths to the individual CSV files
file1 <- "data/ResidentialTransaction20240308160536.csv"
file2 <- "data/ResidentialTransaction20240308160736.csv"
file3 <- "data/ResidentialTransaction20240308161009.csv"
file4 <- "data/ResidentialTransaction20240308161109.csv"
file5 <- "data/ResidentialTransaction20240414220633.csv"

# Reading the individual CSV files
data1 <- read_csv(file1)
data2 <- read_csv(file2)
data3 <- read_csv(file3)
data4 <- read_csv(file4)
data5 <- read_csv(file5)

# Combining the data frames into one
combined_transaction <- bind_rows(data1, data2, data3, data4, data5)

# Viewing the data structure given
col_names <- names(combined_transaction)
col_names
```
:::

Using glimpse to ensure our tibble dataframe is correct

```{r}
glimpse (combined_transaction)

```

### Duplicate checks

::: panel-tabset
## Code

```{r}
#| eval: false
duplicates <- combined_transaction %>% 
  filter(duplicated(.))
glimpse(duplicates)
```

## Output

```{r}
duplicates <- combined_transaction %>% 
  filter(duplicated(.))

glimpse(duplicates)
```
:::

::: callout-tip
## Data Analysis

Using glimpse() as a dipstick to run through our duplicate() checks, we concluded that the data is very sanitised with 0 duplicated transactions.

However we noted that there is an unsuitable data type for the field 'Sale Date' which will result in difficulty for us not being able to do filtering later on.
:::

### Filtering Q1 2024 data for Private properties

::: panel-tabset
## Code

```{r}
#| eval: false

combined_transaction$`Sale Date` <- dmy(combined_transaction$`Sale Date`)

# Check the structure to ensure 'Sale Date' is now a Date object
str(combined_transaction)

Q1_2024_Private <- combined_transaction %>%
  filter(`Sale Date` >= as.Date("2023-01-01") & 
         `Sale Date` <= as.Date("2024-03-31"))

head(Q1_2024_Private)
```

## Date Format Change

```{r}
combined_transaction$`Sale Date` <- dmy(combined_transaction$`Sale Date`)

# Check the structure to ensure 'Sale Date' is now a Date object
str(combined_transaction)

```

## Filtering for only Q1 data

```{r}
Q1_2024_Private <- combined_transaction %>%
  filter(`Sale Date` >= as.Date("2023-01-01") & 
         `Sale Date` <= as.Date("2024-03-31"))
```

## Sample of Q1 Only Data

```{r}
head(Q1_2024_Private)
```
:::

# Data Visualisation

## Transaction Type Distribution across Planning Regions on Different Private Residential Types in 1st Quarter 2024

::: panel-tabset
## Code

```{r}
ggplot(data=Q1_2024_Private, 
       aes(x = `Unit Price ($ PSF)`, 
           fill = `Type of Sale`)) +
  geom_histogram(position = "dodge", binwidth = 100) +  # Adjust binwidth as needed
  facet_grid(`Planning Region` ~ `Property Type`) +
  labs(x = "PSF Price", y = "Count", title = "Distribution of PSF Prices by Property Type, Region, and Transaction Type") +
  scale_fill_brewer(palette = "Set1") +  # Use a color palette that is distinct
  theme_minimal() +
  theme(legend.position = "bottom")  # Adjust legend position as needed
```

## Plot

```{r}
ggplot(data=Q1_2024_Private, 
       aes(x = `Unit Price ($ PSF)`, 
           fill = `Type of Sale`)) +
  geom_histogram(position = "dodge", binwidth = 100) +  # Adjust binwidth as needed
  facet_grid(`Planning Region` ~ `Property Type`) +
  labs(x = "PSF Price", y = "Count", title = "Distribution of PSF Prices by Property Type, Region, and Transaction Type") +
  scale_fill_brewer(palette = "Set1") +  # Use a color palette that is distinct
  theme_minimal() +
  theme(legend.position = "bottom")  # Adjust legend position as needed

```
:::

```{r}
#| eval: false
ggplot(data=Q1_2024_Private, 
       aes(x= `Unit Price ($ PSF)`, 
           fill = `Property Type`)) +
  geom_histogram(bins=20, 
                 color="grey30")

ggplot(data=Q1_2024_Private, 
       aes(x = `Property Type`, 
           fill = `Property Type`)) +
  geom_bar(color = "grey30") +
  labs(x = "Property Type", y = "Count", title = "Count of Property Types") +
  theme_minimal()


ggplot(data=Q1_2024_Private, 
       aes(x = `Unit Price ($ PSF)`)) +
  geom_histogram(bins = 10, fill = "skyblue", color = "black") +
  facet_grid(`Planning Region` ~ `Property Type`, scales = "free_y") +
  labs(x = "PSF Price", y = "Count", title = "Distribution of PSF Prices by Property Type and Region") +
  theme_minimal()


ggplot(data=Q1_2024_Private, 
       aes(x = `Unit Price ($ PSF)`, 
           fill = `Type of Sale`)) +
  geom_histogram(position = "dodge", binwidth = 100) +  # Adjust binwidth as needed
  facet_grid(`Planning Region` ~ `Property Type`) +
  labs(x = "PSF Price", y = "Count", title = "Distribution of PSF Prices by Property Type, Region, and Transaction Type") +
  scale_fill_brewer(palette = "Set1") +  # Use a color palette that is distinct
  theme_minimal() +
  theme(legend.position = "bottom")  # Adjust legend position as needed
```

::: callout-tip
### Observation from visualisation

From the transaction volumes of Property Types segregated by the different regions, we observed that majority of the transactions happening in 2024 involves either Apartments or Condominiums in the Central Region.

Shifting gears into the types of sale, we can also observe that New Sale is very popular for Apartments in Central Region as well as Executive Condominiums in the West Region. This sentiment is also backed by [Straits Times](https://www.straitstimes.com/singapore/housing/first-executive-condo-launch-of-2024-sells-53-of-units)
:::

## Average Transaction Price by Region

To visualise the variation in average property prices across different regions in Singapore, we will use a choropleth map and shade each region according to its average property price which makes pricing differences immediately obvious.

To create the map, we will need the sub-zone boundary of URA Master Plan 2019 dataset as well as our own Q1_2024_Private dataset which we created earlier on.

The tools required are :

```{r}
pacman::p_load(tmap,sf, httr, dplyr, future, furrr)
```

| Components | Description                                                                                                          |
|------------------------------------|------------------------------------|
| tmap       | The syntax for creating plots is similar to that of `ggplot2`, but tailored to maps                                  |
| sf         | Support for simple features, a standardized way to encode spatial vector data                                        |
| httr2      | Create and submit HTTP requests and work with the HTTP responses                                                     |
| future     | For sequential and parallel processing of R expression. This will be useful for expediting processing time later on. |

### Step 1

The first step is to utilise Singapore's OneMap service to map each postal code to their corresponding Longtidue and Latitude.

::: panel-tabset
## Create a new cache layer to store found postal code

```{r}
#| eval: false
cache <- new.env()
plan(multisession)
```

## Create Base API call with a catch clause

```{r}
#| eval: false
fetch_geocode_data <- function(postcode)
  # Check cache first
  if (exists(postcode, envir = cache)) {
    return(get(postcode, envir = cache))
  }
  
  # API parameters
  url <- "https://www.onemap.gov.sg/api/common/elastic/search"
  query_params <- list(searchVal = postcode, returnGeom = 'Y', getAddrDetails = 'Y', pageNum = '1')

  # API call with error handling
  response <- tryCatch({
    GET(url, query = query_params)
  }, error = function(e) {
    message("Error fetching data for postcode ", postcode, ": ", e$message)
    return(NULL)
  })

  # Check if the API call was successful
  if (is.null(response) || http_error(response)) {
    return(data.frame(postcode = postcode, lat = NA, lon = NA))
  }

  # Parse response
  content_data <- content(response, type = "application/json")

  # Store in cache and return results
  if (content_data$found > 0) {
    lat <- content_data$results[[1]]$LATITUDE
    lon <- content_data$results[[1]]$LONGITUDE
    result <- data.frame(postcode = postcode, lat = lat, lon = lon)
  } else {
    result <- data.frame(postcode = postcode, lat = NA, lon = NA)
  }
```

## Store found data in 'result' dataframe

```{r}
#| eval: false
  assign(postcode, result, envir = cache)
  return(result)
```

##Output

```{r}
cache <- new.env()
plan(multisession)

fetch_geocode_data <- function(postcode) {
  # Check cache first
  if (exists(postcode, envir = cache)) {
    return(get(postcode, envir = cache))
  }
  
  # API parameters
  url <- "https://www.onemap.gov.sg/api/common/elastic/search"
  query_params <- list(searchVal = postcode, returnGeom = 'Y', getAddrDetails = 'Y', pageNum = '1')

  # API call with error handling
  response <- tryCatch({
    GET(url, query = query_params)
  }, error = function(e) {
    message("Error fetching data for postcode ", postcode, ": ", e$message)
    return(NULL)
  })

  # Check if the API call was successful
  if (is.null(response) || http_error(response)) {
    return(data.frame(postcode = postcode, lat = NA, lon = NA))
  }

  # Parse response
  content_data <- content(response, type = "application/json")

  # Store in cache and return results
  if (content_data$found > 0) {
    lat <- content_data$results[[1]]$LATITUDE
    lon <- content_data$results[[1]]$LONGITUDE
    result <- data.frame(postcode = postcode, lat = lat, lon = lon)
  } else {
    result <- data.frame(postcode = postcode, lat = NA, lon = NA)
  }

  assign(postcode, result, envir = cache)
  return(result)
}
```
:::

### Step 2

We will now match Transaction (Q1_2024_Private) Tibble Dataframe's unique postal code to our list of postal code extracted from OneMap.

```{r}
# Search 'Q1_2024_Private' dataframe for 'Postal Code' column
data_pc <- unique(Q1_2024_Private$`Postal Code`)

# Use futures for asynchronous processing
results <- future_map(data_pc, fetch_geocode_data)
combined_results <- bind_rows(results)

# Combine results and filter out unsuccessful ones
successful_results <- combined_results %>%
  filter(!is.na(lat) & !is.na(lon))

# Write results to a CSV file
write.csv(successful_results, file = "data/PostalCodeList.csv", row.names = FALSE)
```

```{r}
Q1_2024_Private_with_Coordinates <- Q1_2024_Private %>%
  left_join(successful_results, by = c("Postal Code" = "postcode"))

```

### Step 3 Importing Geospatial Data

Load Map into MPSZ

```{r}
# Instantiate the map from MPSZ 2019
mpsz <- st_read(dsn = "data/",
                layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

```{r}
# Remove rows where latitude or longitude is NA
Q1_2024_Private_with_Coordinates <- Q1_2024_Private_with_Coordinates %>%
  filter(!is.na(lat) & !is.na(lon))

q1_2024_sf <- st_as_sf(Q1_2024_Private_with_Coordinates ,
                       coords = c("lon", "lat"),
                       crs =4326) %>%
  st_transform(crs = 3414)
```

## Extracting study data (Average Transacted Price)

We will filter out the column data - Transacted Price, PSF & Planning Area which is required for a drill-down analysis on consumer pattern.

```{r}
q1_avg_txn <- Q1_2024_Private_with_Coordinates %>%
    group_by(`Planning Area`) %>%
    summarise(Avg_Transacted_Price = mean(`Transacted Price ($)`, na.rm = TRUE))
q1_avg_txn <- q1_avg_txn %>%
    mutate(`Planning Area` = toupper(`Planning Area`))
q1_avg_txn <- st_drop_geometry(q1_avg_txn)

```

```{r}
q1_avg_psf <- Q1_2024_Private_with_Coordinates %>%
    group_by(`Planning Area`) %>%
    summarise(Avg_Transacted_Price = mean(`Unit Price ($ PSF)`, na.rm = TRUE))
q1_avg_psf <- q1_avg_psf %>%
    mutate(`Planning Area` = toupper(`Planning Area`))
q1_avg_psf <- st_drop_geometry(q1_avg_psf)

```

## Geospatial Data Wrangling

::: panel-tabset
## Average Transacted Price ( Planning Region )
```{r}

mpsz_avg_txn_px <- mpsz %>%
    left_join(
        q1_avg_txn,
        by = c("PLN_AREA_N" = "Planning Area")
    ) %>% drop_na()

tmap_mode("view")

map2 <- tm_shape(mpsz_avg_txn_px) +
    tm_polygons(col = "Avg_Transacted_Price", 
                palette = "YlOrRd", 
                alpha = 0.3,
                style = "quantile",
                n = 7) +
    tmap_options(check.and.fix = TRUE) +
  tm_view(set.zoom.limits = c(11,14))

map2


```

## Average PSF ( Planning Region )
```{r}
mpsz_avg_psf <- mpsz %>%
    left_join(
        q1_avg_psf,
        by = c("PLN_AREA_N" = "Planning Area")
    ) %>% drop_na()

tmap_mode("view")

map3 <- tm_shape(mpsz_avg_psf) +
    tm_polygons(col = "Avg_Transacted_Price", 
                palette = "YlOrRd", 
                alpha = 0.3,
                style = "quantile",
                n = 7) +
    tmap_options(check.and.fix = TRUE) +
  tm_view(set.zoom.limits = c(11,14))

map3

```

:::

