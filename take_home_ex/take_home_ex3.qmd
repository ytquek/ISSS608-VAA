---
title: "Take Home 3"
author: "You Ting QUEK"
date: "17 May, 2024"
date-modified: last-modified
execute: 
  eval: true
  echo: true
  warning: false
  freeze: true
---

# **The Source**

The task is taken from the [VAST Challenge 2024](https://vast-challenge.github.io/2024/). Questions from [Mini Case 1](https://vast-challenge.github.io/2024/MC1.html) will be completed.

# **The Task**

Apply appropriate visual analytics methods to help FishEye, a non-profit organization that focuses on illegal fishing, to better identify bias, track behavior changes, and infer temporal patterns from the knowledge graphs prepared by their data analysts.

This take home exercise is done in conjunction with the group project. My group members are [Keke](https://isss608keke.netlify.app/) and [Alicia Loh](https://lnealicia-isss608.netlify.app/).

## Background

In Oceanus, commercial fishing is a major part of the local economy, and the activities of local fishing companies are often discussed in local news reports, especially when there is a major disruption or controversial issue. Lately, the news is filled with stories of companies who are willing to break the law and fish illegally.

FishEye International is an organization whose mission is to discover and stop illegal, unreported, and unregulated fishing. FishEye is an independent non-profit, but they share their findings with law enforcement when warranted. FishEye analysts collect open-source data, including news articles and public reports, and have recently started extracting embedded knowledge from these free text sources using several advanced language models. Knowledge from multiple sources is combined to form CatchNet: the Oceanus Knowledge Graph, which is used to search for evidence of possible illegal fishing activity. Analysts know that data may be biased so they review and edit extracted information before it is added to CatchNet. Data for this challenge includes both the extracted and analyst modified knowledge graph and the original source articles.

Recently, the commercial fishing community in Oceanus was rocked with scandal after SouthSeafood Express Corp was caught fishing illegally. The company's leaders claim an innocent mistake, while others in the industry have suggested getting caught was overdue and is likely part of a larger pattern of suspect behavior. FishEye analysts are aware that a polarizing event like this is likely to attract biased perspectives. They are looking for your help to identify sources of bias in their data. They are having particular trouble determining whether bias comes from the source article, the algorithms used to extract the knowledge graph, or possibly somewhere else...

## Tasks and Questions:

Your task is to develop visual analytics approaches that FishEye analysts can use to verify the facts included in their knowledge graph are representative of facts stated in the source text. Analysts should be able to compare consistency of the extracted knowledge with the source and identify and trace sources of bias in the data. Novel use of large language models (LLMs) as part of a visual analytics process is encouraged.

1.  Use novel visualizations and visual analytic workflows to examine the bias in each news source. Create visualizations to help FishEye analysts understand how bias in the original sources changes over time. You may use the knowledge graph extracts and may use a large language model to supplement your understanding.

2.  FishEye uses two LLM extraction algorithms: ShadGPT and BassLine. Develop visualizations to compare the bias of each algorithm. Though not required, you may develop your own LLM-based extraction and include it in the comparison.

3.  FishEye is also interested in understanding the reliability of their human analysts. Use visual analytics to examine potential analyst bias. Provide visual examples of the types of bias present.

4.  Identify unreliable actors: news sources, algorithms, or analysts. Use visualizations to provide evidence for your conclusions. Can you use the data provided and a visual analytics workflow to determine who else may be involved?

Note: the VAST challenge is focused on visual analytics and graphical figures should be included with your response to each question. Please include a reasonable number of figures for each question (no more than about 6) and keep written responses as brief as possible (around 250 words per question). Participants are encouraged to new visual representations rather than relying on traditional or existing approaches.

# Getting Started

## Installing and loading the required libraries

Note: Ensure that the [pacman](https://cran.r-project.org/web/packages/pacman/) package has already been installed.

The following R packages will be used:

-   tidytext, tidyverse

-   readtext

-   quanteda

```{r}
pacman::p_load(tidytext, readtext, quanteda, tidytext, tidyverse)
```

## **Importing Multiple Text Files**

### **Creating a folder list**

```{r}
data_folder <- "data/articles/"
```

### **Define a function to read all files from a folder into a data frame**

```{r}
text_data <- readtext(paste0("data/articles","/*"))
# alternate version: text_data <- readtext("data/articles/*")
```

Check dataframe

```{r}
corpus_text <- corpus(text_data)
summary(corpus_text, 5)
```

### **Text Data Processing**

-    [`unnest_tokens()`](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) of **tidytext** package is used to split the dataset into tokens

-    [`stop_words()`](https://rdrr.io/cran/tidytext/man/stop_words.html) is used to remove stop-words

```{r}
# Tokenize the text column
unnest_words <- text_data %>%   
  unnest_tokens(output = word, input = text) 

# Filter tokens to include only alphabetic characters and remove stop words
unnest_words <- filter(unnest_words, 
                       str_detect(word, "[a-z']$"),          
                       !word %in% stop_words$word)
```

View dataframe

```{r}
glimpse(unnest_words)
```

### Explore Common words

The code chunk below calculates individual word frequencies to explore common words in the dataset.

```{r}
unnest_words %>%
  count(word,sort = TRUE)
```

::: callout-note
Common words from the articles seem to be related to sustainable fishing practices in the larger fishing industry and trade.
:::

View dataframe

```{r}
glimpse(unnest_words)
```

::: callout-note
In the dataset, it can be seen that under the "doc_id" column, the company mentioned in the article, along with its author is separated by some underscores and numbers. We will split this column into "Company" and "News_Source" to simplify later analysis.
:::

### Split doc_id column

The column has the format **`Company__Number__Number__News_Source.txt`**.

```{r}
text_data_split <- text_data %>%
  separate_wider_delim("doc_id",
                       delim = "__",
                       names = c("Company", "Number", "Number2", "News_Source"),
                       too_many = "debug")
```

View dataframe

```{r}
glimpse(text_data_split)
```

Drop unnecessary columns

```{r}
text_data_clean <- text_data_split %>%
  select(-Number, -Number2, -doc_id, -doc_id_ok, -doc_id_pieces, -doc_id_remainder)
```

View dataframe

```{r}
glimpse(text_data_clean)
```

## EDA

List of Company and News Sources

::: panel-tabset
## Companies

```{r}
# Unique values in the "Company" column
unique_companies <- unique(text_data_clean$Company)
unique_companies
```

## News Sources

```{r}
# Unique values in the "News_Source" column
unique_news_sources <- unique(text_data_clean$News_Source)
unique_news_sources
```
:::

::: callout-note
There are a total of:

-   85 companies being mentioned in the news articles

-   6 news outlets that authored the news articles
:::

Discovering the company that is mentioned the greatest number of times, and the author of the most news articles

::: panel-tabset
## Companies

```{r}
# Count the frequency of values in the "Company" column and sort by frequency
company_freq <- text_data_clean %>%
  count(Company) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  arrange(desc(n))
company_freq
```

## News Sources

```{r}
# Count the frequency of values in the "News_Source" column and sort by frequency
news_source_freq <- text_data_clean %>%
  count(News_Source) %>%
  mutate(Percentage = n / sum(n) * 100) %>%
  arrange(desc(n))
news_source_freq
```
:::

::: callout-note
The top 3 companies with the most mentions in the news articles are:

-   Cervantes-Kramer (12 - 33.1%)

-   Jones GRoup and Rasmussen (12 - 32.2%)

-   Nelson and King (8 - 31.7%)

The top 3 news outlets with the most number of news articles are:

-   The News Buoy (112 - 33.1%)

-   Haacklee Herald (109 - 32.2%)

-   Lomark Daily (107 - 31.7%)
:::
